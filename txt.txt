This section of our lab is dedicated to the Day-2 operations that many administrators would need to
perform when working with virtual machines in their OpenShift Virtualization environment. We will make
use of the understanding we have developed throughout this roadshow of how VMs operate in an
OpenShift environment, and use those skills to complete the tasks in this section. In this particular case,
we are going to work with the three virtual machines that we imported from VMware vSphere earlier in
this roadshow, and we are going to make some minor configuration changes to enable the applications
hosted on those servers to be accessed as they now run in OpenShift Virtualization. To accomplish this,
we will install and configure a loadbalancer, and we will expose our applications using the service/route
method that is the default when making use of the OpenShift SDN pod network so that the application
is reachable from outside of the cluster.
MetalLB concepts
In this portion of the lab, we will review the MetalLB operator and understand how it exposes virtual ma-
chine hosted applications outside of the cluster.
Using MetalLB is valuable when you have a bare-metal cluster or a virtual infrastructure that is treated
like bare-metal, and you want to ensure that there is fault-tolerant access to an application through an
external IP address.
For MetalLB to meet this need, you must configure your networking infrastructure to ensure that the
network traffic for the external IP address is routed from clients to the host network for the cluster.
It can operate in two modes:
MetalLB operating in layer2 mode provides support for failover by utilizing a mechanism similar to
IP failover. However, instead of relying on the virtual router redundancy protocol (VRRP) and
keepalived, MetalLB leverages a gossip-based protocol to identify instances of node failure. When
a failure is detected, another node assumes the role of the leader node, and a gratuitous ARP mes-
sage is dispatched to broadcast this change.
MetalLB operating in layer3 or border gateway protocol (BGP) mode delegates failure detec-
tion to the network. The BGP router or routers that the OpenShift nodes have established a con-
nection with will identify any node failure and terminate the routes to that node.
Using MetalLB instead of IP failover is often preferable for ensuring high availability of pods and
services.
Layer 2 mode
In layer 2 mode, the speaker pod on one node announces the external IP address for a service to the
host network. From a network perspective, the node appears to have multiple IP addresses assigned to
a network interface.
In layer 2 mode, all traffic for a service IP address is routed through one node. After traffic enters the
node, the service proxy for the CNI network provider distributes the traffic to all the pods for the
service.
When a node becomes unavailable, failover is automatic. The speaker pods on the other nodes detect
that a node is unavailable, and a new speaker pod on a surviving node will take ownership of the service
IP address from the failed node.

Layer 3 (BGP) mode
In BGP mode, by default, each speaker pod advertises the load balancer IP address for a service to each
BGP peer. It is also possible to advertise the IPs coming from a given pool to a specific set of peers by
adding an optional list of BGP peers. BGP peers are commonly network routers that are configured to
use the BGP protocol. When a router receives traffic for the load balancer IP address, the router picks
one of the nodes with a speaker pod that advertised the IP address. The router sends the traffic to that
node. After traffic enters the node, the service proxy for the CNI network plugin distributes the traffic to
all the pods for the service.
If a node becomes unavailable, the router then initiates a new connection with another node that has a
speaker pod that is advertising the load balancer IP address.

Navigate to Operators → Installed Operators. Click on the the Project: dropdown and select the
metallb-system namespace.
Configure MetalLB
With our MetalLB application successfully installed it’s now time to configure it for our use.
For this portion of the lab, we will use the same network where the OpenShift Cluster nodes are located
(192.168.123.0/24) and for this exercise we will reserve the IP range 192.168.123.200-192.168.123.250 to
be used for load balanced services in the OpenShift cluster.
Create IPAddress Pool
The first step is to create an IP address pool to assign IPs to applications to be accessed from outside
our cluster.
1. Click on the tab for IPAddressPool click on the button for Create IPAddressPool.
Use the name ip-addresspool-webapp and under section addresses, remove any existing ad-
dresses and enter 192.168.123.200-192.168.123.250 as the address pool. When complete it should
look similar to this image:
